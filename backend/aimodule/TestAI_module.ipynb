{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "xGkk3xzF-hg1",
        "outputId": "7336e207-10ff-4149-974f-718fa2ebebf2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'open_clip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-997fda504e0b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mopen_clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'open_clip'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import open_clip\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Încarcă modelul și tokenizer-ul CLIP\n",
        "model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(\n",
        "    'hf-hub:laion/CLIP-ViT-B-32-laion2B-s34B-b79K'\n",
        ")\n",
        "tokenizer = open_clip.get_tokenizer('hf-hub:laion/CLIP-ViT-B-32-laion2B-s34B-b79K')\n",
        "\n",
        "# 2. Definirea transformărilor pentru preprocesarea imaginii\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),  # Transformă imaginea în RGB (3 canale)\n",
        "    transforms.Resize((224, 224)),  # Redimensionează la dimensiunea necesară pentru CLIP\n",
        "    transforms.ToTensor(),          # Convertește imaginea într-un tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalizarea imaginii\n",
        "])\n",
        "\n",
        "# 3. Încarcă dataset-ul FashionMNIST\n",
        "def load_fashion_mnist():\n",
        "    # Încarcă setul de date FashionMNIST (train sau test)\n",
        "    dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "    return dataloader\n",
        "\n",
        "# 4. Crează embedding pentru text\n",
        "def process_text(text):\n",
        "    # Tokenizează și creează embedding pentru text\n",
        "    text_input = tokenizer([text])\n",
        "    text_features = model.encode_text(text_input)\n",
        "    return text_features\n",
        "\n",
        "# 5. Compară imaginea și textul folosind similaritatea cosine\n",
        "def compare_image_and_text(image_features, text_features):\n",
        "    # Calculăm similaritatea cosine între embedding-urile imaginii și textului\n",
        "    similarity = F.cosine_similarity(image_features, text_features)\n",
        "    return similarity.item()\n",
        "\n",
        "# 6. Aplica modelul pe un set de imagini și compară cu descrierea textului\n",
        "def apply_model_on_fashion_mnist(model, dataloader, description, max_images=100):\n",
        "    results = []\n",
        "    image_count = 0  # Contor pentru a limita procesarea la 100 de imagini\n",
        "\n",
        "    # Creăm embedding pentru descrierea textului\n",
        "    text_features = process_text(description)\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        if image_count >= max_images:  # Oprește procesarea după 100 de imagini\n",
        "            break\n",
        "\n",
        "        # Procesează fiecare imagine din batch\n",
        "        for i in range(images.size(0)):\n",
        "            if image_count >= max_images:  # Verifică dacă am procesat deja 100 de imagini\n",
        "                break\n",
        "\n",
        "            image = images[i].unsqueeze(0)  # Adaugă o dimensiune batch\n",
        "\n",
        "            with torch.no_grad():  # Evită calculul gradientului pentru inferență\n",
        "                # Obține embedding-ul imaginii\n",
        "                image_features = model.encode_image(image)\n",
        "\n",
        "                # Compară imaginea cu textul\n",
        "                similarity = compare_image_and_text(image_features, text_features)\n",
        "\n",
        "                results.append({\n",
        "                    'image': labels[i].item(),  # Eticheta (de exemplu, 0 pentru T-shirt, 1 pentru trouser etc.)\n",
        "                    'similarity': similarity,\n",
        "                    'image_tensor': images[i]  # Salvează tensorul imaginii pentru vizualizare\n",
        "                })\n",
        "\n",
        "            image_count += 1  # Crește contorul de imagini procesate\n",
        "\n",
        "    return results\n",
        "\n",
        "# 7. Vizualizarea imaginilor\n",
        "def show_image(image_tensor):\n",
        "    # Convertește tensorul în imagine folosind matplotlib\n",
        "    image = image_tensor.permute(1, 2, 0)  # Permută dimensiunile pentru a fi compatibile cu matplotlib\n",
        "    image = image.numpy()  # Convertește tensorul într-un array numpy\n",
        "    plt.imshow(image)  # Afișează imaginea\n",
        "    plt.axis('off')  # Ascunde axele\n",
        "    plt.show()\n",
        "\n",
        "# 8. Testează modelul pe setul de imagini FashionMNIST cu o descriere\n",
        "if __name__ == \"__main__\":\n",
        "    # Încarcă setul de date FashionMNIST\n",
        "    dataloader = load_fashion_mnist()\n",
        "\n",
        "    # Descrierea textului de căutat\n",
        "    description = \"T-shirt\"  # Schimbă descrierea în funcție de testul dorit\n",
        "\n",
        "    # Aplică modelul pe setul de imagini FashionMNIST și compară cu descrierea\n",
        "    results = apply_model_on_fashion_mnist(model, dataloader, description)\n",
        "\n",
        "    # Afișează primele 10 rezultate și vizualizează imaginile\n",
        "    for result in results[:10]:  # Afișăm primele 10 rezultate\n",
        "        print(f\"Eticheta imaginei: {result['image']} - Similaritate cu '{description}': {result['similarity']}\")\n",
        "        # Vizualizează imaginea\n",
        "        show_image(result['image_tensor'])\n"
      ]
    }
  ]
}